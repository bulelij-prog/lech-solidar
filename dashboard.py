"""
NExUS v2.5 - Dashboard RAG avec Vertex AI Discovery Engine
RÃ©gion EU - HiÃ©rarchie des normes belges - RÃ¨gle de faveur
"""

import streamlit as st
import json
import logging
from datetime import datetime
from typing import Optional, List, Dict, Tuple
from google.oauth2 import service_account
import vertexai
from vertexai.generative_models import GenerativeModel
from google.cloud import discoveryengine_v1
from google.cloud.discoveryengine_v1.services.search_service import SearchServiceClient

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ================================================================
# CONFIGURATION - RÃ‰GION EU
# ================================================================

st.set_page_config(
            page_title="NExUS v2.5 - Protocoles CHU Brugmann",
            page_icon="âš–ï¸",
            layout="wide",
            initial_sidebar_state="expanded"
)

# Configuration Discovery Engine - RÃ‰GION EUROPE
PROJECT_ID = "syndicat-novembre-2025"
LOCATION = "eu"  # ğŸ‡ªğŸ‡º RÃ‰GION EUROPE
DATA_STORE_ID = "nexus-cgsp-pdf-global"
HOSPITAL_FILTER = "iris_brugmann"

# HiÃ©rarchie des normes belges
DOC_TYPE_HIERARCHY = {
            "Loi": 3,
            "CCT": 2,
            "Protocole": 1
}

# Prompt systÃ¨me - RÃ¨gle de faveur
SYSTEM_PROMPT = """Tu es NExUS, assistant juridique expert pour les dÃ©lÃ©guÃ©s syndicaux du CHU Brugmann.

ğŸ”´ RÃˆGLES ABSOLUES (application stricte):
1. âš–ï¸ RÃˆGLE DE FAVEUR: En cas de doute, interprÃ¨te TOUJOURS en faveur du travailleur/dÃ©lÃ©guÃ©
2. ğŸ‡§ğŸ‡ª HIÃ‰RARCHIE BELGE: Loi > CCT > Protocole (applique cet ordre d'interprÃ©tation)
3. ğŸ“‹ SOURCES: Cite obligatoirement [DOC-1], [DOC-2] etc. 
4. âŒ PAS D'INVENTION: RÃ©ponds UNIQUEMENT basÃ© sur les documents fournis
5. ğŸ’¯ PRÃ‰CISION: ZÃ©ro hallucination, rigueur maximale

Si l'information n'est pas dans les documents, dis-le clairement.
Structure ta rÃ©ponse avec titres, listes Ã  puces, rÃ©fÃ©rences explicites.
"""

# ================================================================
# INITIALISATION VERTEX AI
# ================================================================

@st.cache_resource
def initialize_vertex_ai():
            """Initialise Vertex AI avec Service Account"""
            try:
                            service_account_json = st.secrets.get("GCP_SERVICE_ACCOUNT_JSON")
                            if not service_account_json:
                                                st.error("âŒ GCP_SERVICE_ACCOUNT_JSON manquant")
                                                st.stop()

                            credentials_dict = json.loads(service_account_json)
                            credentials = service_account.Credentials.from_service_account_info(
                                credentials_dict
                            )

        project_id = credentials_dict.get("project_id")
        if not project_id:
                            st.error("âŒ project_id introuvable")
                            st.stop()

        vertexai.init(project=project_id, credentials=credentials)
        return project_id, credentials

except Exception as e:
        st.error(f"âŒ Erreur Vertex AI: {str(e)}")
        st.stop()


@st.cache_resource
def get_search_client():
            """Retourne le client Discovery Engine"""
            try:
                            return SearchServiceClient()
except Exception as e:
        logger.error(f"Erreur SearchServiceClient: {str(e)}")
        return None


# ================================================================
# MOTEUR RAG - RECHERCHE DANS LE DATA STORE
# ================================================================

def search_datastore(
            query: str,
            doc_type_filter: Optional[str] = None,
            max_results: int = 5
) -> Tuple[List[Dict], str]:
            """
                Recherche sÃ©mantique avec hiÃ©rarchie des normes
                    RÃ©gion: EU
                        Filtre: hopital_id = iris_brugmann
                            """
            try:
                            client = get_search_client()
                            if not client:
                                                return [], "Erreur: Client Discovery Engine indisponible"

                            # Chemin de la ressource
                            serving_config = (
                    f"projects/{PROJECT_ID}/locations/{LOCATION}/"
                                                f"collections/default_collection/dataStores/{DATA_STORE_ID}"
                            )

        # Filtre
        filter_str = f'hospital_id = "{HOSPITAL_FILTER}"'
        if doc_type_filter:
                            filter_str += f' AND doc_type = "{doc_type_filter}"'

        # RequÃªte
        request = discoveryengine_v1.SearchRequest(
                            serving_config=serving_config,
                            query=query,
                            page_size=max_results,
                            filter=filter_str,
                            content_search_spec={
                                                    "snippet_spec": {
                                                                                "max_snippet_length": 500,
                                                                                "reference_only": False,
                                                    },
                                                    "summary_spec": {
                                                                                "summary_result_count": 5,
                                                                                "use_semantic_chunks": True,
                                                    },
                            },
        )

        response = client.search(request)

        # Extraction
        documents = []
        for result in response.results:
                            doc = result.document
                            struct_data = doc.struct_data if hasattr(doc, 'struct_data') else {}

            doc_entry = {
                                    'filename': struct_data.get('file_name', 'Sans titre'),
                                    'doc_type': struct_data.get('doc_type', 'Protocole'),
                                    'url': doc.uri if hasattr(doc, 'uri') else "",
                                    'snippet': result.snippet.snippet_status if hasattr(result, 'snippet') else "",
                                    'relevance_score': getattr(result, 'relevance_score', 0.0),
                                    'hierarchy_priority': DOC_TYPE_HIERARCHY.get(
                                                                struct_data.get('doc_type', 'Protocole'), 0
                                    )
            }
            documents.append(doc_entry)

        # TRI: HiÃ©rarchie > Pertinence
        documents.sort(
                            key=lambda x: (-x['hierarchy_priority'], -x['relevance_score'])
        )

        # Context formatÃ©
        context = "## ğŸ“š DOCUMENTS PERTINENTS (HiÃ©rarchie belge appliquÃ©e)\n---\n"
        for idx, doc in enumerate(documents, 1):
                            context += f"\n**[DOC-{idx}] {doc['filename']}** ({doc['doc_type']})\n"
                            context += f"â€¢ Pertinence: {doc['relevance_score']:.0%}\n"
                            context += f"â€¢ Contenu: {doc['snippet'][:400]}...\n"
                        context += "\n---\n"

        logger.info(f"âœ“ {len(documents)} docs trouvÃ©s")
        return documents, context

except Exception as e:
        logger.error(f"âŒ Erreur search_datastore: {str(e)}")
        return [], f"Erreur recherche: {str(e)}"


def call_gemini_with_rag(
            prompt: str,
            model_name: str,
            rag_context: str
) -> str:
            """
                Appelle Gemini avec RAG + RÃ¨gle de faveur
                    Temperature: 0.1
                        """
    try:
                    augmented_prompt = f"""{SYSTEM_PROMPT}

                    {rag_context}

                    ## ğŸ“Œ QUESTION
                    {prompt}

                    âš ï¸ Rappel: Applique la RÃˆGLE DE FAVEUR et la HIÃ‰RARCHIE BELGE.
                    """

        model = GenerativeModel(model_name=model_name)
        response = model.generate_content(
                            augmented_prompt,
                            generation_config={
                                                    "max_output_tokens": 2048,
                                                    "temperature": 0.1,
                            }
        )

        return response.text if response.text else "Pas de rÃ©ponse"

except Exception as e:
        logger.error(f"Erreur Gemini: {str(e)}")
        return f"Erreur: {str(e)}"


# ================================================================
# INITIALISATION
# ================================================================

project_id, _ = initialize_vertex_ai()

# ================================================================
# UI - INTERFACE PRINCIPALE
# ================================================================

st.title("âš–ï¸ NExUS v2.5")
st.markdown("**Assistant juridique - Protocoles CHU Brugmann**")
st.markdown(f"ğŸ‡ªğŸ‡º RÃ©gion: EU | ğŸ“Š Projet: {project_id}")
st.divider()

# SIDEBAR
with st.sidebar:
            st.header("âš™ï¸ Configuration")

    model = st.selectbox(
                    "ModÃ¨le Gemini:",
                    ["gemini-2.0-flash", "gemini-1.5-pro"],
                    index=0
    )

    st.divider()
    st.subheader("ğŸ” Recherche")

    doc_filter = st.selectbox(
                    "Filtrer par type:",
                    ["Tous", "Loi", "CCT", "Protocole"],
                    index=0
    )
    doc_type = None if doc_filter == "Tous" else doc_filter

    max_docs = st.slider("Docs Ã  consulter:", 1, 10, 5)

    st.divider()
    st.info("âš–ï¸ **RÃ¨gle de faveur**: ACTIVE")
    st.info("ğŸ‡§ğŸ‡ª **HiÃ©rarchie**: Loi > CCT > Protocole")
    st.info("ğŸ”’ **Temperature**: 0.1 (Rigueur)")

# ZONE CHAT
st.subheader("ğŸ’¬ Posez votre question")

# Historique
if "messages" not in st.session_state:
            st.session_state.messages = []

# Afficher historique
for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                            st.markdown(msg["content"])

# Chat input
user_input = st.chat_input(
            "Ex: Quels sont mes droits de formation professionnelle?",
            key="chat_input"
)

if user_input:
            # Ajouter Ã  l'historique
            st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("user"):
                    st.markdown(user_input)

    # Recherche + Analyse
    with st.chat_message("assistant"):
                    with st.spinner("ğŸ” Recherche dans les protocoles..."):
                                        documents, rag_context = search_datastore(
                                                                query=user_input,
                                                                doc_type_filter=doc_type,
                                                                max_results=max_docs
                                        )

                    if documents:
                                        with st.spinner("â³ Analyse (Gemini, RÃ¨gle de faveur)..."):
                                                                response = call_gemini_with_rag(
                                                                                            prompt=user_input,
                                                                                            model_name=model,
                                                                                            rag_context=rag_context
                                                                )

                                        st.markdown(response)

            # Sources
                        st.markdown("---")
            st.subheader("ğŸ“š Sources consultÃ©es")

            for idx, doc in enumerate(documents, 1):
                                    icon = "âš–ï¸" if doc['doc_type'] == "Loi" else "ğŸ“‹" if doc['doc_type'] == "CCT" else "ğŸ“„"
                                    st.markdown(
                                        f"{icon} **[DOC-{idx}]** {doc['filename']} "
                                        f"*({doc['doc_type']})* â€” {doc['relevance_score']:.0%}"
                                    )

            # Sauvegarde
            st.session_state.messages.append({"role": "assistant", "content": response})
else:
            st.warning("âš ï¸ Aucun document trouvÃ©")

st.divider()
st.caption("NExUS v2.5 | RÃ©gion EU | HiÃ©rarchie belge | RÃ¨gle de faveur")
